{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8db19cd",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "- `pip install hvplot`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441caf5d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Class will start at 6:05 PM\n",
    "\n",
    "In the meantime... fun fact!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4678989",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- 'Bold Glamour' filter on TikTok uses AI and not AR https://www.theverge.com/2023/3/2/23621751/bold-glamour-tiktok-face-filter-beauty-ai-ar-body-dismorphia\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96fe938",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We‚Äôre getting to a new place where it‚Äôs not just augmenting reality, it‚Äôs replacing reality.\n",
    "- It uses GAN (Generative Adversarial Networks):  \"GANs pit two competing neural networks against each other in a fist-fight\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d70594",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 3: Model setup and intro to regression models\n",
    "### Intro to Machine Learning | Professional Certificate course \n",
    "\n",
    "Viviana Marquez,  M.Sc.<br>\n",
    "March 8, 2023\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src='../img/all/di.jpeg' style='width:500px; float: left; margin: 0px 30px 15px 0px'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccaef5d0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ‚ùáÔ∏è Housekeeping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcf078d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Office hours\n",
    "\n",
    "More office hours:\n",
    "- Friday, March 10 @ 12PM PST\n",
    "- Monday, March 13 @ 6PM PST\n",
    "- Friday, March 17 @ 12PM PST\n",
    "- Monday, March 20 @ 6PM PST\n",
    "- Friday, March 24 @ 12PM PST\n",
    "- Monday, March 27 @ 6PM PST\n",
    "- Monday, April 3 @ 6PM PST\n",
    "\n",
    "Zoom link on Canvas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8b7cd3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# üöÄ Today's agenda\n",
    "\n",
    "**Part 1:**\n",
    "\n",
    "- Finish up Feature Engineering\n",
    "- Underfitting vs overfitting / Bias-Variance trade-off"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f31e72",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# üöÄ Today's agenda\n",
    "\n",
    "**Part 2:**\n",
    "- Linear Regression\n",
    "- RMSE / R squared"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfccbcd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ‚è™ Last class recap\n",
    "\n",
    "- Getting data\n",
    "- Pandas crash course\n",
    "- EDA: Exploratory Data Analysis\n",
    "- Train, validation, and test data sets\n",
    "- Data cleaning and feature engineering "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f7445a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Conda environment commands\n",
    "\n",
    "- `conda activate IntroML`\n",
    "    - Launch `jupyter notebook`\n",
    "    \n",
    "    \n",
    "### Alternative\n",
    "\n",
    "Google Colab: https://colab.research.google.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f5c083",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# üöÄ Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf92e74",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ‚ùáÔ∏è Finish up Feature Engineering\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img src='../img/class_3/cinderella.gif' style='height:350px;'>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfad8d4f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# EDA\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img src='../img/class_2/pipeline_EDA.png' style='height:300px;'>\n",
    "</center>\n",
    "\n",
    "- Exploratory Data Analysis is the process of performing initial investigations on data to be able to understand it, identify patters and relationships between variables, detect outliers and anomalies with the help of summary statistics and data visualizations.\n",
    "\n",
    "- By exploring the data in this way, data professionals can check the overall health of their data and gain insights and make informed decisions about how to approach further analysis or modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f13e17e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Feature Engineering \n",
    "<br>\n",
    "<center>\n",
    "    <img src='../img/class_2/pipeline_FE.png' style='height:300px;'>\n",
    "</center>\n",
    "\n",
    "- Feature engineering is the process of selecting and transforming raw data into features that can be used to train machine learning models\n",
    "- AKA creating/changing number of columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e828306",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è We need split the data into train/validation/test data sets!\n",
    "\n",
    "- It is important to separate the data into training, validation, and test sets **BEFORE** doing any data cleaning or feature engineering to prevent <u>data leakage</u> and ensure that the model is evaluated on unseen data.\n",
    "\n",
    "\n",
    "- **Training dataset:** Used to train machine learning model\n",
    "- **Validation dataset:** Used to tune the model's hyperparameters \n",
    "- **Test datatset:** Evaluate the model's performance on unseen data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727770b4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src='../img/class_2/cali.png' style='height:500px; float: left; margin: 0px 100px 0px 0px'>\n",
    "\n",
    "\n",
    "Download data from [here](https://raw.githubusercontent.com/vivianamarquez/Intro-to-Machine-Learning/main/data/housing.csv)\n",
    "\n",
    " - California Housing Prices dataset\n",
    "     - 1990 California census\n",
    "     \n",
    "     \n",
    "Task: Create a model to predict median house value based on the other features of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd651d3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c98d85",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c4ea62",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95cc09b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c33a4f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a1e104",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc8c3d2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f90fd1f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d7b2ee77",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Why do we do data cleaning and feature engineering?\n",
    "\n",
    "<br><br>\n",
    "<center>\n",
    "    <img src='../img/class_0/gigo.png' style='height:300px;'>\n",
    "</center>\n",
    "\n",
    "- Objective: Improve data quality, making it suitable for modeling\n",
    "- All data must be numeric and there can't be any missing values (even with purely numeric data, there is potential cleanup work)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9237ffbe",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# üõÅ Data cleaning\n",
    "\n",
    "- Included but not limited to: \n",
    "    - Handling missing values: drop them or account for them \n",
    "    - Handling outliers: drop them or account for them or keep them\n",
    "    - Remove duplicates\n",
    "    - Handling incorrect data types\n",
    "    - Handling inconsistent data (example: age shouldn't be negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd798c5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# üèó Feature engineering \n",
    "\n",
    "- Included but not limited to: \n",
    "    - Handling categorical attributes\n",
    "            - Ordinal encoder: Assumes two nearby values are more similar to each other\n",
    "            - One-Hot encoder: One binary category per attribute\n",
    "    - Custom feature engineering  ‚ôä\n",
    "    - ‚ú® Feature scaling \n",
    "            - MinMax \n",
    "            - Standarization \n",
    "            \n",
    "- ‚ú® Putting it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c76b521",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Feature Scaling\n",
    "\n",
    "- With few exceptions, machine learning algorithms don't perform well when the input of the numerical values have very different scales\n",
    "- Example in this dataset: `total_rooms`, `median_income`\n",
    "\n",
    "The most common ones:\n",
    "- MinMax scaling\n",
    "- Standarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e3d7a9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### MinMax\n",
    "\n",
    "- AKA normalization\n",
    "- Values are shifted and rescaled so that they end up ranging from 0 to 1\n",
    "- How to calculate it? Subtracting the min val and diving by the difference between the min and the max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3be6053",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d14bd9b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4850ca54",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Standarization\n",
    "\n",
    "- How to calculate it? Subtract the mean value, then divide the result by the standard deviation\n",
    "- It doesn't restrict the values to a specific range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be73398",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12cc6d2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8ee0e778",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ü§î When would you use standarization vs MinMax?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19a3025",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d8deea0e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Suppose a district has a median income of 100 (by mistake)\n",
    "- MinMax would map this outlier to 1 and squish all other values to 0-0.15\n",
    "- Meanwhile, standarization is not so sensitive to outliers\n",
    "- MinMax is better to use when the upper and lower boundaries are known"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b18986c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### More notes on feature scaling\n",
    "\n",
    "- When a feature's distribution has a heavy tail, both MinMax and standarization will squash most values into a small range\n",
    "- You should first transform it to shrink the heavy tail \n",
    "    - Square root\n",
    "    - Log\n",
    "    - Bucket the feature\n",
    "    - and more..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77325e7e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "def67ccc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# üèó Feature engineering \n",
    "\n",
    "- Included but not limited to: \n",
    "    - Handling categorical features: Ordinal encoder, OneHot encoder, etc.\n",
    "    - Feature scaling: MinMax, standarizaton, square root, log, etc.\n",
    "    - Feature extraction: Creating new features from existing ones\n",
    "    - Text processing: TF-IDF, Word2Vec, BERT, GPT\n",
    "    - Dimensionality reduction\n",
    "- Depends on your dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd23f2d0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ‚öóÔ∏è Putting all of it together\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866ed1c5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ce755c94",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Impute the median to handle missing values\n",
    "- One-Hot Encoder to handle categorical values\n",
    "- Standartization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e154db",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c1dfa4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f3c50a1e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ü§î Do you notice something funny in here?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d8e306",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ‚ùáÔ∏è Underfitting vs overfitting / Bias-Variance trade-off"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ebb6c7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Is our model any good? \n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img src='../img/class_3/overfit_underfit.webp' style='height:300px;'>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3f425c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A good model makes useful predictions on unknown, future data (it **generalizes**)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec730cec",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- It might not be very accurate, but it's better than what coin flip, might still be useful"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc360bbb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- ‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è We always need 3 data sets: train, val, test\n",
    "- Test set: Used **only** after you think you have the best model. No peeking!!! üëÄ It's the only true measure of generality\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa0c9d5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- If model is inaccurate on training set, the model is **biased**\n",
    "- If model is inaccurate on test set, the model doesn't generalize (high **variance**)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e88fd09",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Resource**: https://elitedatascience.com/bias-variance-tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e6f625",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Sources of prediction error\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img src='../img/class_3/error_types.png' style='height:300px;'>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bd7dfd",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We are given $(X,y)$ training data and we fit a model $\\hat{f}(X)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144e4f9d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- $Err = (\\hat{f}(x^{(i)})-y^{(i)})^2$ from a single observation in the test case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113cbf5a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- There are three sources of errors in that $Err$ number:\n",
    "    - Noisy $X$ or $y$ data, such as inconsistent $X \\to y$\n",
    "    - Model underfitting or bias: Too weak of simple\n",
    "    - Model overfitting: Model too specific to training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a44e58",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Conceptually: $Err$ = \"noise\" + \"bias\" + \"overfitting\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e7c9c9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Stats nerds: $Err$ = Irreducible Error + Bias^2 + Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5b1b24",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1. Noise can lead to inconsistent data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d32d5f2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Imagine you have two observations such as:\n",
    "    - $[18,1,9] \\to 91$\n",
    "    - $[18,1,9] \\to 99$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d01a4fe",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- No model can predict two different $y$ values for the same $x$ vector\n",
    "- Model will have $Err>0$ no matter what"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43526fc3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We know this as **irreducible error**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbacd64",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Noise comes from faulty sensors, typos, self-reporting issues, etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480e8b73",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- ü§∑ Nothing we can do about the irreducible error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa44706",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1. Noise can lead to inconsistent data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0cb241",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- What if inconsistent training observations, such as:\n",
    "    - $[18,1,9] \\to 91$\n",
    "    - $[18,1,9] \\to 99$\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d886207",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- were really just missing a variable we don't have?\n",
    "    - $[18,1,9,10] \\to 91$\n",
    "    - $[18,1,9,7] \\to 99$\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9a2496",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Example: Two apartment observations look identical: 2bd & 1bath, but they have very different price only because we lack \"awesome views\" vars\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd9ca1b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Missing variables are called *exogenous* variables\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0953a92",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 2. Overly simple models lead to biased models\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img src='../img/class_3/underfit2.png' style='height:300px;'>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7321834d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Bias** is the error rate of your model on the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b719bb",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Bias is how much your model **underfits** the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0394ab3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How do you compute bias?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3fe91d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "Bias[\\hat{f}(x)] = E[\\hat{f}(x)-y]\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf15566",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Expected difference between predicted and observed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d2149a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# üëÆ‚Äç‚ôÄÔ∏è Check-in\n",
    "\n",
    "A model that has a good ability to fit the training data has ___________ bias.\n",
    "\n",
    "{high,low}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9113f469",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bias is bad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49405dbe",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A model that has a good ability to fit the training data has **low** bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db85b115",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We want to minimize bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795a0248",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Models with high bias:\n",
    "    - Fail to capture meaningful patterns in data\n",
    "    - Under-fit training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0be88b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How to decrease bias?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdaf277",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Make the model more complex \n",
    "    - Add more parameters\n",
    "    - Pick a different model\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1a1c02",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 3. Overly complex models can overfit\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img src='../img/class_3/overfit2.png' style='height:300px;'>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe3316e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Variance** is the amount a model's prediction will change if a different training data is used, ie, small changes in the training data can result in large changes in the estimated model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b968c1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Variance** in a model is the flexibility to learn patters in the observed data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1573d9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Variance is how much your model **overfits** the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9fcb77",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How do you compute variance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26aa3e20",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "Var[\\hat{f}(x)] = E[\\hat{f}(x)^2] - (E[\\hat{f}(x)])^2\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983c5d09",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Intuitively, how much the algorithm will move around its mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c87363",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# üëÆ‚Äç‚ôÄÔ∏è Check-in\n",
    "\n",
    "A model that is strongly influenced by the specifics of the training data has ___________ variance.\n",
    "\n",
    "{high,low}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8994737f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A model that is strongly influenced by the specifics of the training data has **high** variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237e48f2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Model Complexity will Increase Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03617a10",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The more complex the model is, the more data points it will \"capture\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb03152a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- However, complexity will make the model \"move\" more to \"capture\" the data points, and hence its variance will be larger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5436762",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A model that has a good ability to predict test data has **low** variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731336d5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We want to minimize variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cff89c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How to decrease variance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca0f81b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A larger training set tends to decrease variance, ie, reduces the chance of overfitting --> increases the chance of generalization \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1520a94e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Regularization\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe13cb2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Make the model less complex\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949745be",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bias-variance trade-off\n",
    "\n",
    "### So... we need to decrease both bias and variance (to avoid underfitting and overfitting)\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img src='../img/class_3/both.gif' style='height:300px;'>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed3868c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Must increase the complexity of the model to get more accuracy (reduce variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c491e0ff",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- But, increased complexity means more ability to chase quirks of data (high variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54831b01",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Conversely, if we reduce the model's complexity, we'll increase bias and reduce variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89951ff",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- This is why it's called a trade-off"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4e1cf0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What to do?\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img src='../img/class_3/bias_variance.png' style='height:300px;'>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b09b33a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Let the validation be your guide to approximate complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a6a91f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. Collect a lot of data\n",
    "1. Engineer good features\n",
    "1. Pick a complex algorithm\n",
    "1. Train the specific model until validation scores starts to go down (smart early stopping)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd32408b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Summary\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img src='../img/class_3/summary.png' style='height:750px;'>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7505719",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# üöÄ Part 2\n",
    "\n",
    "See you here at @"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c507d4f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ‚ùáÔ∏è Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f94344f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# üó∫Ô∏èüìç Guide map \n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img src='../img/class_3/pipeline_train.png' style='height:350px;'>\n",
    "</center>\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe781589",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# üó∫Ô∏èüìç Guide map \n",
    "\n",
    "<center>\n",
    "    <img src='../img/class_3/reg.png' style='height:550px; float: center; margin: 0px 0px 0px 0px'>\n",
    "</center>\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501267ee",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Regressor vs Classifier; 2 sides of the same coin\n",
    "<br>\n",
    "<center>\n",
    "    <img src='../img/class_3/reg_class.png' style='height:400px; float: center; margin: 0px 0px 0px 0px'>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d8080c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- If target is numerical, model is a regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4df896",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- If target is categorical, model is a classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5b698d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Regressors draw through data, classifiers draw between clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e76401b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How does training, prediction look like in code?\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img src='../img/class_3/scikitlearn.png' style='height:300px; float: center; margin: 0px 0px 0px 0px'><br>\n",
    "    <small>Image credits: T. Parr</small>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62101bb0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- LinearRegression and RandomForest are objects representing models and hyperparameters go in as args to constructor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22869c3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Why do we study linear models?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de62e2a2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Simple, interpretable, super fast, can't be beat for linear relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8e16e2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Usually, a lower bound on performance but they often form the basis of other more powerful techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583aa703",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Combining multiple linear models into a lattice with a nonlinear function as glue yields a **neural network**; those are insanely useful and powerful üî•üî•üî•"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b923a97",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What problem are we solving?\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img src='../img/class_3/pizza.png' style='height:300px; float: center; margin: 0px 0px 0px 0px'><br>\n",
    "    <small>Image and example credits: T. Parr</small>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384a976a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Imagine you're in college and you're given a fixed $500 for food every month"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9242fd8e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- If you wanted to know, at current rate of pizza consumption, how fast you‚Äôd run out of money you can plot it and ‚Äúeyeballed‚Äù zero $ùë•$ point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf7792d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# We use linearity all the time without realizing it\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeab3c4d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- If we are baking cookies and the recipe calls for 2 cups of flour and we want to\n",
    "double the recipe, then we know that we need 4 cups of flour. If we want to triple\n",
    "the recipe, then we need 6 cups of flour. The amount of flour needed depends\n",
    "linearly on the number of recipes we are making."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18b6802",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- If we buy one Starbucks Grande coffee for `$2.10` we know that if we buy our friend\n",
    "one too, it will cost us `$4.20`. The cost depends linearly on the number of cups\n",
    "we purchase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bc411e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- What does ‚Äúdepend linearly‚Äù mean graphically? We know that when we plot a linear function we get a straight line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bb79f6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# üëÆ‚Äç‚ôÄÔ∏è Check-in\n",
    "\n",
    "Below are some plots of functions; determine in which cases $y$ depends\n",
    "linearly on $x$.\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img src='../img/class_3/check.png' style='height:300px; float: center; margin: 0px 0px 0px 0px'><br>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2237c47b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Preparing data for linear regression "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c039d07d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Linear regression is been studied at great length, and there is a lot of literature on how your data must be structured to make the best use of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb207e1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- As such, there is a lot of sophistication when talking about these requirements and expectations which can be intimidating. In practice, you can use these rules more as rules of thumb when using Ordinary Least Squares Regression, the most common implementation of linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf1521a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- **Linear Assumption:** Linear regression assumes that the relationship between your input and output is linear. It does not support anything else. This may be obvious, but it is good to remember when you have a lot of attributes. You may need to transform data to make the relationship linear (e.g. log transform for an exponential relationship).\n",
    "\n",
    "- **Remove Noise:** Linear regression assumes that your input and output variables are not noisy. Consider using data cleaning operations that let you better expose and clarify the signal in your data. This is most important for the output variable and you want to remove outliers in the output variable (y) if possible.\n",
    "\n",
    "- **Remove Collinearity:** Linear regression will over-fit your data when you have highly correlated input variables. Consider calculating pairwise correlations for your input data and removing the most correlated.\n",
    "\n",
    "- **Gaussian Distributions:** Linear regression will make more reliable predictions if your input and output variables have a Gaussian distribution. You may get some benefit using transforms (e.g. log or BoxCox) on your variables to make their distribution more Gaussian looking.\n",
    "\n",
    "- **Rescale Inputs:** Linear regression will often make more reliable predictions if you rescale input variables using standardization or normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d071bc6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Let's code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acaade51",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790a80b9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad53b3a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8311290",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006c9881",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "63dbe4c6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è `fit()` vs `fit_transform()` vs `transform()`\n",
    "\n",
    "Scikit_Learn's API is remarkably well designed. \n",
    "\n",
    "- `fit()` and `fit_transform()` should **ONLY** be used on training data\n",
    "- Use `transform()` on any other set, including val, test, and new data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb3ee01",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Let's explore our linear regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e903fe",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Intercept \n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img src='../img/class_3/y_intercept.webp' style='height:300px; float: center; margin: 0px 0px 0px 0px'><br>\n",
    "</center>\n",
    "\n",
    "\n",
    "Interpret the y-intercept of a line as the value of y when x equals to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8f3768",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "316547d7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Coefficients  \n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img src='../img/class_3/lr_formula.png' style='height:300px; float: center; margin: 0px 0px 0px 0px'><br>\n",
    "</center>\n",
    "\n",
    "\n",
    "- A positive coefficient indicates that as the value of the independent variable increases, the mean of the dependent variable also tends to increase.\n",
    "\n",
    "- A negative coefficient suggests that as the independent variable increases, the dependent variable tends to decrease.\n",
    "- Your goal with linear regression is finding those optimal coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d279b9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aeed2997",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Interpreting the coefficients:\n",
    "<br>\n",
    "<center>\n",
    "    <img src='../img/class_3/change.png' style='height:300px; float: center; margin: 0px 0px 0px 0px'><br>\n",
    "</center>\n",
    "\n",
    "Holding all other features fixed, a 1 unit increase in `population` is associated with an decrease of $42,456"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54a10a2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "!pip install hvplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b559e502",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f29808",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2c424aba",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# A note on categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47e20b7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "If we only use one criteria in the training set, then we might expect that the result is\n",
    "not too good. Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa9925d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Suppose we are using the square feet of living space as our criteria in the training set.\n",
    "In this case we expect that if we have two homes with 2800 sq ft of living space, 4 bedrooms, and 2 baths then\n",
    "the algorithm will predict the same listing price. However if"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52afd68f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The first home is located in a prestigious neighborhood, is new construction, has a pool, 4 bedrooms, 2 baths, and a 2-car garage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4785846d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- and the second home was built in 1930, located on a dirt road, and has 4 bedrooms,\n",
    "2 baths, but no garage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75559113",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# üëÄ Spoiler alerts: regression evaluation metrics and regularized linear models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435ff392",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a371467",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "85d0d4f5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ‚è™ Today's recap\n",
    "\n",
    "- High Bias: Model is inaccurate on training set (too simple)\n",
    "- High Variance: Model is too good on training set, but inaccurate on test set (aka doesn't generalize well, too complex)\n",
    "- Bias-variance trade-off: We want both low bias and low variance but when one goes down, the other one goes up\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28a7a07",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# üëÆ‚Äç‚ôÄÔ∏è Misc:\n",
    "\n",
    "- Self-graded quiz will be posted in the next hour\n",
    "- üí™ Ungraded homework will be posted tonight or tomorrow morning\n",
    "- Office hours on Friday at 12 PM PST and Monday 6 PM PST (Zoom link will be posted in Canvas) \n",
    "\n",
    "### Check-in: https://forms.gle/iPGEKNwJP1c8ftSp8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2900de",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<img src='../img/all/bye.gif' style='height:400px;'> \n",
    "</center>\n",
    "\n",
    "# Next class: \n",
    "- Regression models part 2 \n",
    "    - Gradient descent\n",
    "    - Regularized linear models\n",
    "    - Regression performance metrics: RMSE / R Squared\n",
    "- Intro to classification models"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
