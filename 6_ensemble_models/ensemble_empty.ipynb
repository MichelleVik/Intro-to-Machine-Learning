{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "544ad31f",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "- `pip install autoviz`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ae2109d",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441caf5d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Class will start at 6:05 PM\n",
    "\n",
    "In the meantime... fun fact!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0beaaf25",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- AutoViz: Automatically Visualize any dataset, any size with a single line of code ü§Øü§Øü§Ø"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e6d81f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7b36d1",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "12d70594",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 6: Ensemble models and model selection/optimization/interpretation\n",
    "### Intro to Machine Learning | Professional Certificate course \n",
    "\n",
    "Viviana Marquez,  M.Sc.<br>\n",
    "March 29, 2023\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src='../img/all/di.jpeg' style='width:500px; float: left; margin: 0px 30px 15px 0px'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8b7cd3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# üöÄ Today's agenda\n",
    "\n",
    "**Part 1:**\n",
    "\n",
    "- Ensemble Learning\n",
    "    - Voting classifiers\n",
    "    - Bagging\n",
    "    - Boosting\n",
    "    - Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f31e72",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# üöÄ Today's agenda\n",
    "\n",
    "**Part 2:**\n",
    "\n",
    "- Model selection and optimization\n",
    "    - Hyperparameter tuning\n",
    "        - Cross-Validation\n",
    "        - Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfccbcd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ‚è™ Last class recap\n",
    "\n",
    "- Classification models continuation \n",
    "    - Logistic regression\n",
    "    - Support Vector Machines (SVM)\n",
    "    - Naive-Bayes\n",
    "    \n",
    "- Non-parametric models\n",
    "    - KNN\n",
    "    - Decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f7445a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Conda environment commands\n",
    "\n",
    "- `conda activate IntroML`\n",
    "    - Launch `jupyter notebook`\n",
    "    \n",
    "    \n",
    "### Alternative\n",
    "\n",
    "Google Colab: https://colab.research.google.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f5c083",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# üöÄ Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a481656",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ‚ùáÔ∏è Ensemble Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e944a0b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Decision trees\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img src='../img/class_5/dt.gif' style='height:350px; float: center; margin: 0px 0px 0px 0px'>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1907821b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- It is an algorithm that can be used for both classification and regression\n",
    "- The goal is to create a model that predicts the value of the target variable by learning simple decision rules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f74c55",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Decision trees\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37af1eaa",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### üëç Pros \n",
    "\n",
    "- The predictions can be interpreted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d9547d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### üëé Cons\n",
    "\n",
    "- Prone to overfitting\n",
    "- Unstable, but that can be solved with an ensemble model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1324895",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "    <img src='../img/class_5/spoiler.jpg' style='height:350px; float: center; margin: 0px 0px 0px 0px'>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8296e13e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Random forests \n",
    "\n",
    "<center>\n",
    "    <img src='../img/class_5/rf.png' style='height:350px; float: center; margin: 0px 0px 0px 0px'>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a597dbf",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eca5a74",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba4c124",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1a6bdd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2583dda",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e2df2523",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Let's go back to our real estate agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf8a7e7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The real estate agent predicts a house price in their head by visiting lots of houses in SF and taking the average of comp places"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81a42ba",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Downside: less precise house predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c48c93",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Hmm...can we think of another way to average the price? ü§î "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3869f57",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Wisdom of the crowd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14752e58",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Recruit multiple real estate agents to build house price models in their heads by visiting lots of houses; then each agent can estimate prices of unvisited houses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a97ec7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- An agent's prediction might not be accurate: the prediction for one house might be too low but a for another house might be too high"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec3100c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Averaging all agents‚Äô predictions is better than the prediction of a single agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73930aeb",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src='../img/class_6/this2.png' style='width:250px; float: left; margin: 0px 30px 15px 0px'> <b>Ensemble Learning!</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716c88f1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Ensemble Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e17028",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Aggregating the predictions of a group of predictors (such as classifiers or regressors), you will often get better predictions than with the best individual predictor.\n",
    "\n",
    "Types of ensemble methods:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c5090b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Voting classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d29b30",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Bagging and Pasting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36992e7b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d112d1e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0682a2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##  Voting classifiers\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img src='../img/class_6/voting.png' style='height:350px; float: center; margin: 0px 0px 0px 0px'>\n",
    "</center>\n",
    "\n",
    "- Train a few classifiers and aggregate their predictions: Predict the class that gets the most votes\n",
    "- Even if each classifier is a *weak learner*, the ensemble can still be a *strong learner*\n",
    "- Needs: Sufficient weak learners and sufficiently diverse"
   ]
  },
  {
   "cell_type": "raw",
   "id": "653a16be",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "# Scale data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "min_max_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X_train = min_max_scaler.fit_transform(X_train)\n",
    "X_test = min_max_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b02edd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4b2416",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69c5a84",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa20c62",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a72493",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cbbffb0e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##  Bagging (bootstrap aggregating) and pasting\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img src='../img/class_6/bagging.png' style='height:350px; float: center; margin: 0px 0px 0px 0px'>\n",
    "</center>\n",
    "\n",
    "- Use the same classifier for each predictor, but train them on different random subsets of the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1216dbe8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Bagging: Sampling is performed **with** replacement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4388f8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Pasting: Sampling is performed **without** replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0ee9f8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d17e1737",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##  Example of bagging: Random Forest\n",
    "\n",
    "- Ensemble of Decision Trees (usually through bagging)\n",
    "\n",
    "<center>\n",
    "    <img src='../img/class_5/rf.png' style='height:350px; float: center; margin: 0px 0px 0px 0px'>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca0bdbd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6fcc8212",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The fundamental difference between Random Forest and bagging of Decision Trees is that in Random forests, only a subset of features are selected at random out of the total and the best split feature from the subset is used to split each node in a tree, unlike in bagging where all features are considered for splitting a node\n",
    "\n",
    "\n",
    "To simulate RF using bagging, initialize the DT as:\n",
    "\n",
    "```python\n",
    "\n",
    "dt = DecisionTreeClassifier(splitter='random')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f2610a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##  Boosting\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img src='../img/class_6/boosting.png' style='height:350px; float: center; margin: 0px 0px 0px 0px'>\n",
    "</center>\n",
    "\n",
    "- Use the same classifier for each predictor, but train them **sequentially**, each tying to correct its predecessor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e336b88a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##  Examples of Boosting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d30336",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "- **AdaBoost**: Pays a bit more attention to the training instances that the predecessor underfitted. This results in new predictors focusing more and more on the hard class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78818753",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "AdaBoost notes: Simply put, the idea is to set weights to both classifiers and data points (samples) in a way that forces classifiers to concentrate on observations that are difficult to correctly classify . This process is done sequentially in that the two weights are adjusted at each step as iterations of the algorithm proceed. For example, to build an AdaBoost classifier, a first base classifier (such as a Decision Tree) is trained and used to make predictions on the training set. The relative weight of misclassified training instances is then increased. A second classifier is trained using the updated weights and again it makes predictions on the training set, weights are updated, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2738fd",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Gradient Boosting**: Also works sequentially but instead tries to fit the new predictor to the residual errors made by the previous predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0d8b78",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **XGBoost** (Extreme Gradient Boosting): Optimized implementation of Gradient Boosting "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8a56d9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "    <img src='../img/class_6/meme.jpeg' style='height:700px; float: center; margin: 0px 0px 0px 0px'>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44393479",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e18468",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a4a5d48d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##  Stacking\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img src='../img/class_6/stacking.png' style='height:350px; float: center; margin: 0px 0px 0px 0px'>\n",
    "</center>\n",
    "\n",
    "- Instead of using trivial functions (such as hard voting) to aggregate the predictions of all predictors in an ensemble, why don't we train a model to perform this aggregation? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c19bef",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "- Hard voting: Majority class\n",
    "- Soft voting: Averages out the probabilities calculated by individual algorithms (can only be obtained with classifiers that can calculate probabilities of the outcome, such as Naive-Bayes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ad5c88",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "- Ensemble models will never increase bias/variance and most of the time will decrease it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7505719",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# üöÄ Part 2\n",
    "\n",
    "See you here at @"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56d5592",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ‚ùáÔ∏è Model selection and optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09209a5a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# üó∫Ô∏èüìç Guide map \n",
    "\n",
    "<center>\n",
    "    <img src='../img/class_6/ml_map_sup.png' style='height:550px; float: center; margin: 0px 0px 0px 0px'>\n",
    "</center>\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d751c558",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ü§î What else can I do to improve my model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5080c876",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Hyperparameter tunning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e00fa1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "    - Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0ab094",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "    - Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46aa2f98",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce19a3f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Parameters**\n",
    "    - Configuration variable that is internal to the model and is used to make predictions on new data\n",
    "    - Model parameters are learned from training data\n",
    "    - They are often not set manually by you"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6682080c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Examples of parameters**\n",
    "    - The coefficients of linear regression models\n",
    "    - Support vectors in SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c871fc",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Hyperparameters**\n",
    "    - Configuration variables that are set before the training process begins\n",
    "    - They control the behavior of the learning algorithm and the model itself (ie determine parameters)\n",
    "    - Cannot be learned directly from the training data\n",
    "    - They are often specified by you"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74783bc9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Examples of hyperparameters**\n",
    "    - The $k$ in $k$-nearest neighbors\n",
    "    - Regularization hyperparameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb0aa01",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hyperparameter tunning\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img src='../img/class_6/radio.jpeg' style='height:300px; float: center; margin: 0px 0px 0px 0px'>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93eb91c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The process of finding the best combination of hyperparameters for a given machine-learning model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b23c190",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The process: Try out different values for each hyperparameter and evaluate the resulting model's performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be0e97f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Think of it as turning the knows of a radio to get a clear signal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbce7b31",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ü§î How do we do hyperparameter tunning? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53127292",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- So evaluating a model is simple enough: just use a test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bb5e9a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- How do you choose the best hyperparameter? One option could be to train 100 different models using 100 different values for a hyperparameter and pick the best one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8d4135",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **ISSUE**: With this approach you're measuring the generalization error multiple times on the test set, therefore, adapted the model and hyperparameters to produce the best model for that particular set. Result: Model is unlikely to perform as well on new data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4f0f42",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Solution**: Simply hold out part of the training set to evaluate several candidate models and select the one that performs the best on the validation set, then evaluate on the test set to get an estimate of how your model would perform in the real world"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb84c650",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src='../img/class_6/this2.png' style='width:250px; float: left; margin: 0px 30px 15px 0px'> <b>Validaiton set!</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496e501f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Some hyperparameter strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ff0784",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a4abf1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e6f436",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847362b5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Different Model Evaluation Procedures \n",
    "-----\n",
    "\n",
    "1. Training and testing on the same data\n",
    "1. Train/test split\n",
    "1. Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcd4b8d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Cross-Validation \n",
    "-----\n",
    "\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img src='../img/class_6/crossval.png' style='height:400px; float: center; margin: 0px 0px 0px 0px'>\n",
    "</center>\n",
    "\n",
    "1. Randomly split the training set into $k$ distinct subsets called folds\n",
    "2. Train and evaluate the model $k$ times, picking a different fold for evaluation every time and training on the other $k-1$ folds\n",
    "3. **Holdout method**: The final model is tested on the completely hold-out test data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3297326f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Common uses of Cross-Validation\n",
    "-----\n",
    "\n",
    "- Compare different features \n",
    "- Compare different hyperparameters\n",
    "- Compare different algorithms\n",
    "- Estimate Variance (e.g., average model performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8fe259",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7aa613",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e74e33f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3950db53",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc3679a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1807ec",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df6fec0a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Grid search\n",
    "\n",
    "AKA just try everything!\n",
    "\n",
    "- `GridSearchCV`: More efficient parameter tuning  \n",
    "- Allows you to define a grid of parameters that will be searched using $k$-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c1735a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97eac5ad",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b2797b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6854920f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "    <img src='../img/class_6/gs.png' style='height:400px; float: center; margin: 0px 0px 0px 0px'>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1f2798",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- You can reduce computational expense using `RandomizedSearchCV`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fd6f96",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- State-of-the-art algorithms for sampling hyperparameters: Optuna https://optuna.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d0d4f5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ‚è™ Today's recap\n",
    "\n",
    "- Ensemble Learning\n",
    "    - Voting classifiers\n",
    "    - Bagging\n",
    "        - Random Forest\n",
    "    - Boosting\n",
    "        - AdaBoost\n",
    "        - Gradient Boosting\n",
    "        - XGBoost\n",
    "    - Stacking\n",
    "    \n",
    "- Model selection and optimization\n",
    "    - Hyperparameter tuning\n",
    "        - Cross-Validation\n",
    "        - Grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28a7a07",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# üëÆ‚Äç‚ôÄÔ∏è Misc:\n",
    "\n",
    "- Self-graded quiz will be posted in the next hour\n",
    "- üí™ Ungraded homework will be posted tonight or tomorrow morning\n",
    "- Office hours **only** on Monday 6 PM PST (Zoom link is posted in Canvas) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2900de",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<img src='../img/all/bye.gif' style='height:400px;'> \n",
    "</center>\n",
    "\n",
    "# Next class: \n",
    "- Unsupervised learning\n",
    "    - K-means and distance metrics\n",
    "    - Hierarchical clustering\n",
    "    - PCA\n",
    "- Model interpretation\n",
    "- Farewell\n",
    "    - Brief note on data literacy/data ethics\n",
    "    - Student requests: https://forms.gle/5jLyPJDeJ7NjFNps7\n",
    "    - End of the course survey"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
